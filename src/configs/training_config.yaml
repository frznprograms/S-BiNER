pre-training:
  - experiment: 1
    learning_rate: 2e-5
    batch_size: 8
    epochs: 5
    mplm: "mdeberta-v3-base"
    threshold: 0.5
    warmup_ratio: 0.1
    weight_decay: 1e-2
    logging_steps: 100
    save_strategy: "epoch"
    gradient_checkpointing: True
    mixed_precision: "fp32"
    model_save_path: "output"

