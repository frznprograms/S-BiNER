{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19775aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to Python path: /Users/shaneryan_1/Downloads/binary_align_zh\n",
      "Current working directory: /Users/shaneryan_1/Downloads/binary_align_zh/src/datasets\n",
      "✅ 'src' directory found - imports should work now\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "current_dir = Path.cwd()\n",
    "project_root = (\n",
    "    current_dir.parent.parent if current_dir.name == \"datasets\" else current_dir\n",
    ")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Added to Python path: {project_root}\")\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Verify the fix worked\n",
    "if (project_root / \"src\").exists():\n",
    "    print(\"✅ 'src' directory found - imports should work now\")\n",
    "else:\n",
    "    print(\"❌ 'src' directory not found - check your project structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47bca8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shaneryan_1/Downloads/binary_align_zh/src/datasets\n"
     ]
    }
   ],
   "source": [
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0b03c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaneryan_1/Downloads/binary_align_zh/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from transformers import XLMRobertaTokenizer, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee42fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Constantin', 'o', 'ple']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Constantinople\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a971727d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " '晓',\n",
       " '美',\n",
       " '焰',\n",
       " '▁',\n",
       " '来到',\n",
       " '▁',\n",
       " '北京',\n",
       " '立',\n",
       " '方',\n",
       " '庭',\n",
       " '▁',\n",
       " '参观',\n",
       " '▁',\n",
       " '自然',\n",
       " '语',\n",
       " '义',\n",
       " '科技',\n",
       " '公司']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"晓美焰 来到 北京立方庭 参观 自然语义科技公司\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82886c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " '晓',\n",
       " '美',\n",
       " '焰',\n",
       " '▁',\n",
       " '来到',\n",
       " '▁',\n",
       " '北京',\n",
       " '立',\n",
       " '方',\n",
       " '庭',\n",
       " '▁',\n",
       " '参观',\n",
       " '▁',\n",
       " '自然',\n",
       " '语',\n",
       " '义',\n",
       " '科技',\n",
       " '公司']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\n",
    "    [\"晓美焰\", \"来到\", \"北京立方庭\", \"参观\", \"自然语义科技公司\"],\n",
    "    is_split_into_words=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d472b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = AlignmentDatasetGold(\n",
    "#     tokenizer=XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\"),\n",
    "#     source_lines_path=\"../../data/raw_data/english.txt\",\n",
    "#     target_lines_path=\"../../data/raw_data/chinese.txt\",\n",
    "#     alignments_path=\"../../data/raw_data/alignment.txt\",\n",
    "#     one_indexed=True,\n",
    "#     save=False,\n",
    "#     limit=21999,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d80d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dca679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb02fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e1a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"../../data/cleaned_data/aa_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f48a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "110dd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_ten = data.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de302f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_ten[\"source\"] = first_ten[\"final\"].apply(lambda x: x.split(\" ||| \")[0])\n",
    "# first_ten[\"target\"] = first_ten[\"final\"].apply(lambda x: x.split(\" ||| \")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30b876c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_ten.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "735493a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_ten[\"source\"].head(800000).to_csv(\n",
    "#     path_or_buf=\"../../data/cleaned_data/train.src\", sep=\"\\t\", header=False, index=False\n",
    "# )\n",
    "# first_ten[\"source\"].tail(200000).to_csv(\n",
    "#     path_or_buf=\"../../data/cleaned_data/dev.src\", sep=\"\\t\", header=False, index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382f995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_ten[\"target\"].head(800000).to_csv(\n",
    "#     path_or_buf=\"../../data/cleaned_data/train.tgt\", sep=\"\\t\", header=False, index=False\n",
    "# )\n",
    "# first_ten[\"target\"].tail(200000).to_csv(\n",
    "#     path_or_buf=\"../../data/cleaned_data/dev.tgt\", sep=\"\\t\", header=False, index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ebe4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_alignments = pd.read_csv(\n",
    "#     \"../../data/cleaned_data/awesome_alignments.txt\", sep=\"\\t\", header=None\n",
    "# ).head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3b58204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_alignments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5676efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_alignments.head(800000).to_csv(\n",
    "#     path_or_buf=\"../../data/cleaned_data/train.talp\",\n",
    "#     sep=\"\\t\",\n",
    "#     header=False,\n",
    "#     index=False,\n",
    "# )\n",
    "# sample_alignments.tail(200000).to_csv(\n",
    "#     path_or_buf=\"../../data/cleaned_data/dev.talp\",\n",
    "#     sep=\"\\t\",\n",
    "#     header=False,\n",
    "#     index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a9e9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.configs.dataset_config import DataLoaderConfig, DatasetConfig\n",
    "from src.configs.model_config import ModelConfig\n",
    "from src.configs.train_config import TrainConfig\n",
    "from transformers import AutoTokenizer\n",
    "from src.models.binary_align_trainer import BinaryAlignTrainer\n",
    "from src.datasets.datasets_silver import AlignmentDatasetSilver\n",
    "from src.utils.helpers import collate_fn_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "352e9c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:49:35.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.base_dataset\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mPreparing dataset...\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 25/25 [00:00<00:00, 171.48it/s]\n",
      "\u001b[32m2025-07-17 15:49:36.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.decorators\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mFunction executed in: 0 hours, 0 minutes, 0.170 seconds\u001b[0m\n",
      "100%|██████████| 25/25 [00:00<00:00, 194.00it/s]\n",
      "\u001b[32m2025-07-17 15:49:36.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.decorators\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mFunction executed in: 0 hours, 0 minutes, 0.130 seconds\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:36.194\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.datasets.datasets_silver\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m15\u001b[0m - \u001b[32m\u001b[1mAlignmentDatasetSilver initialized successfully\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:36.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.base_dataset\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mPreparing dataset...\u001b[0m\n",
      "100%|██████████| 5/5 [00:00<00:00, 37.74it/s]\n",
      "\u001b[32m2025-07-17 15:49:36.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.decorators\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mFunction executed in: 0 hours, 0 minutes, 0.133 seconds\u001b[0m\n",
      "100%|██████████| 5/5 [00:00<00:00, 50.76it/s]\n",
      "\u001b[32m2025-07-17 15:49:36.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.decorators\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mFunction executed in: 0 hours, 0 minutes, 0.099 seconds\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:36.959\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.datasets.datasets_silver\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m15\u001b[0m - \u001b[32m\u001b[1mAlignmentDatasetSilver initialized successfully\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:36.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mInitialising BinaryAlignTrainer...\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:36.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mSet device to mps.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:36.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mSet seed to None.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:36.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mLoaded configuration objects.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:37.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_factory\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mCreating RoBERTa model for FacebookAI/roberta-base\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.009\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m61\u001b[0m - \u001b[32m\u001b[1mModel initialized.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.009\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m62\u001b[0m - \u001b[32m\u001b[1mBinaryAlignTrainer initialised.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mStarting training...\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mInitialising accelerator...\u001b[0m\n",
      "/Users/shaneryan_1/Downloads/binary_align_zh/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:496: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "\u001b[32m2025-07-17 15:49:39.013\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m78\u001b[0m - \u001b[33m\u001b[1mMismatch in training devices: \n",
      "Accelerator device: mps\n",
      "User specified device: mps.\n",
      "Accelerator device will be used.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.013\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m87\u001b[0m - \u001b[32m\u001b[1mAccelerator initialised.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mAdding special context token...\u001b[0m\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "\u001b[32m2025-07-17 15:49:39.477\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m97\u001b[0m - \u001b[32m\u001b[1mSpecial tokens added.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mInitialising optimizer...\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.478\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m138\u001b[0m - \u001b[32m\u001b[1mOptimizer initialised.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mInitialising learning rate scheduler.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.478\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m144\u001b[0m - \u001b[32m\u001b[1mLearning rate scheduler initialised.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m147\u001b[0m - \u001b[1mAccelerator preparing training components...\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.683\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m161\u001b[0m - \u001b[32m\u001b[1mAccelerator prepared training components.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:39.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m164\u001b[0m - \u001b[1mStarting training loop...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " Num examples =  144\n",
      " Num Epochs =  5\n",
      " Batch Size per device =  32\n",
      " Total batches per epoch =  9\n",
      " Total optimization steps =  45\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:15<00:00,  2.85it/s]\n",
      "\u001b[32m2025-07-17 15:49:55.488\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.models.binary_align_trainer\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m238\u001b[0m - \u001b[32m\u001b[1mTraining completed successfully.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:49:55.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.decorators\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mFunction executed in: 0 hours, 0 minutes, 16.630 seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_config = ModelConfig(model_name_or_path=\"FacebookAI/roberta-base\")\n",
    "train_config = TrainConfig(experiment_name=\"trainer-test\", mixed_precision=\"no\")\n",
    "train_dataset_config = DatasetConfig(\n",
    "    source_lines_path=\"../../data/cleaned_data/train.src\",\n",
    "    target_lines_path=\"../../data/cleaned_data/train.tgt\",\n",
    "    alignments_path=\"../../data/cleaned_data/train.talp\",\n",
    "    limit=25,\n",
    ")\n",
    "eval_dataset_config = DatasetConfig(\n",
    "    source_lines_path=\"../../data/cleaned_data/dev.src\",\n",
    "    target_lines_path=\"../../data/cleaned_data/dev.tgt\",\n",
    "    alignments_path=\"../../data/cleaned_data/dev.talp\",\n",
    "    limit=5,\n",
    "    do_inference=True,\n",
    ")\n",
    "dataloader_config = DataLoaderConfig(collate_fn=collate_fn_span)\n",
    "tok = AutoTokenizer.from_pretrained(model_config.model_name_or_path)\n",
    "train_data = AlignmentDatasetSilver(tokenizer=tok, **train_dataset_config.__dict__)\n",
    "eval_data = AlignmentDatasetSilver(tokenizer=tok, **eval_dataset_config.__dict__)\n",
    "\n",
    "trainer = BinaryAlignTrainer(\n",
    "    tokenizer=tok,\n",
    "    model_config=model_config,\n",
    "    train_config=train_config,\n",
    "    dataset_config=train_dataset_config,\n",
    "    dataloader_config=dataloader_config,\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    seed_num=1,\n",
    ")\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99b388b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc48c827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'bpe2wordmap'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data.data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d7a7678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  0.,  0.,  1.,  1.,\n",
       "         2.,  2.,  2.,  2.,  3.,  3.,  3.,  3.,  3.,  4.,  4.,  4.,  4.,  5.,\n",
       "         5.,  5.,  6.,  6.,  7.,  7.,  7.,  8.,  9.,  9.,  9., 10., 10., 11.,\n",
       "        11., 11., 11., 11., 12., 12., 13., 13., 13., 13., 13., 14., 15., 15.,\n",
       "        15., 15., 15., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16., 16., 17., 17., 18., 18., 18., 18., 19., -1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data.data[0][\"bpe2wordmap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "711c54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc323e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binary-align-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
